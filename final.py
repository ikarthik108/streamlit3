# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XXfwqm6H4rikdVG4kMJG193b90S-lkRu
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

df=pd.read_csv('cleaned.csv')






df.rename(columns={'SEX':'GENDER','MARRIAGE':'MARITAL_STATUS'},inplace=True)



#Sampling the Dataset before modelling
from imblearn.combine import SMOTETomek
from imblearn.over_sampling import RandomOverSampler

X=df.drop('Default_Status',axis=1)
y=df['Default_Status']





# Implementing Oversampling for Handling Imbalanced 
smk = SMOTETomek(random_state=40)
X_res,y_res=smk.fit_sample(X,y)

#Converting numpy array to DF
sampled_X=pd.DataFrame(X_res)
sampled_y=pd.DataFrame(y_res)

sampled_df=pd.concat([sampled_X,sampled_y],axis=1)

# adding column name to the respective columns 
# sampled_X.columns =X.columns
# sampled_y.columns =y.columns

sampled_df.columns=df.columns

sampled_df

#Lets Create A feature vector representing how far is the Bill amount from the Limit Balance 

#Normalizing This by dividing with the Limit Balance.

sampled_df['Closeness_6'] = (sampled_df.LIMIT_BAL - sampled_df.BILL_AMT6)
sampled_df['Closeness_5'] = (sampled_df.LIMIT_BAL - sampled_df.BILL_AMT5)   
sampled_df['Closeness_4'] = (sampled_df.LIMIT_BAL - sampled_df.BILL_AMT4)  
sampled_df['Closeness_3'] = (sampled_df.LIMIT_BAL - sampled_df.BILL_AMT3) 
sampled_df['Closeness_2'] = (sampled_df.LIMIT_BAL - sampled_df.BILL_AMT2)
sampled_df['Closeness_1'] = (sampled_df.LIMIT_BAL - sampled_df.BILL_AMT1)  

sampled_df.describe()
z=sampled_df['Default_Status']
sampled_df.drop("Default_Status",axis=1,inplace=True)
sampled_df=pd.concat([sampled_df,z],axis=1)
sampled_df.to_csv("sampled.csv",index=False)

"""# Weight Of Evidence"""

newdf=pd.DataFrame()
def get_WoE(df,col,bins=5):
  target='Default_Status'
  if col not in cat_cols:
    binned_col =pd.qcut(df[col],bins,duplicates='drop')
    d0=pd.DataFrame({'x':binned_col,'y':df[target]})
  else:
    d0=pd.DataFrame({'x':df[col],'y':df[target]})
  d = d0.groupby("x", as_index=False).agg({"y": ["count", "sum"]})
  d.columns = ['Cutoff' + col, 'N', 'Events']
  d['Non_Events']=d['N'] - d['Events']
  d['% of Events'] = np.maximum(d['Events'], 0.5) / d['Events'].sum()
  d['% of Non_Events'] = np.maximum(d['Non_Events'], 0.5) / d['Non_Events'].sum()
  d['WoE']=np.log(d['% of Events']/d['% of Non_Events'])
  d['IV'] = d['WoE'] * ( d['% of Events'] - d['% of Non_Events'] )
  total_iv=d['IV'].sum()
 
  # print(d)
  return total_iv

cat_cols=['GENDER','EDUCATION','MARRITAL_STATUS','PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']
total_cols=list(sampled_df.columns)
total_cols.remove('Default_Status')
numeric_cols=set(total_cols) - set(cat_cols)
numeric_cols=list(numeric_cols)

total_cols

woe=pd.DataFrame()
iv_vals=list()
for col in total_cols:
  IV=get_WoE(sampled_df,col)
  iv_vals.append(IV)


woe['Features']=total_cols
woe['Information Value']=pd.Series(iv_vals)

woe.sort_values(by='Information Value',ascending=False)

woe_features=list(woe['Features'][woe['Information Value']>=0.1])

woe_features

def one_hot_encoder():

  sampled_df['GENDER']=sampled_df['GENDER'].replace({1:'Male',2:'Female'})
  sampled_df['MARITAL_STATUS']=sampled_df['MARITAL_STATUS'].replace({1:'Married',2:'Single',3:'Others'})
  sampled_df['EDUCATION']=sampled_df['EDUCATION'].replace({1:'Graduate',2:'University', 3:'High school', 4:'Others'})

#We are one hot encoding these features because otherwise the model may assume that they have some linear relation with the target
  ohe_features=['EDUCATION','GENDER','MARITAL_STATUS']

#Saving these dummies in a dataframe
  dummies=pd.get_dummies(sampled_df[ohe_features])

  sampled_df=pd.concat([sampled_df,dummies],axis=1)


  sampled_df.drop(ohe_features,axis=1,inplace=True)

sampled_df

#!pip install catboost
#importing all the model libraries
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from mlxtend.classifier import StackingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import VotingClassifier
import xgboost as xgb
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
import lightgbm as lgb
from catboost import CatBoostClassifier

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn import model_selection
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import ShuffleSplit
from sklearn.metrics import classification_report
from sklearn.metrics import precision_score,recall_score,f1_score
from sklearn.metrics import roc_curve, roc_auc_score

#Simple Logistic Regression
lr_model=LogisticRegression(random_state=1,max_iter=400)


#MAX VOTING classifier
model1=DecisionTreeClassifier(random_state=0)
model2=KNeighborsClassifier(n_neighbors=1)
model3=LogisticRegression(random_state=1,max_iter=400)
np.random.seed(18)
model4=GaussianNB()

#Hard Voting
Voting_model=VotingClassifier(estimators=[('dt',model1),('knn',model2),('lr',model3)],voting='hard') #Every individual model votes for a class,considers majority vote.

#In soft voting, every individual classifier provides a probability value that a specific data point belongs to a particular target class. 
#The predictions are weighted by the classifier's importance and summed up. Then the target label with the greatest sum of weighted probabilities wins the vote.
eclf1= VotingClassifier(estimators=[('dt',model1),('knn',model2),('lr',model3)],voting='soft')
eclf2 = VotingClassifier(estimators=[('dt',model1),('knn',model2),('GNB',model4)],voting='soft') 
eclf3 = VotingClassifier(estimators=[('dt',model1),('lr',model3),('GNB',model4)],voting='soft') 
eclf4 = VotingClassifier(estimators=[('knn',model2),('lr',model3),('GNB',model4)],voting='soft') 




#XGBoost
xgb_model1 = xgb.XGBClassifier(objective="binary:logistic" ,seed=42,learning_rate=0.01)

xgb_model2 = xgb.XGBRFClassifier(n_estimators=100, subsample=0.9, colsample_bynode=0.2)

gb_model = GradientBoostingClassifier(random_state=34)

catboost_model=CatBoostClassifier()



#Stacking Different Models and using logistic regression as a meta classifier
clf1 = KNeighborsClassifier()
clf2 = RandomForestClassifier(n_estimators=50,random_state=1)
clf3 = GaussianNB()
clf4 = LogisticRegression(random_state=1,max_iter=300)
lr = lr_model




sclf1 = StackingClassifier(classifiers=[clf1, clf2, clf3], 
                          use_probas=True,
                          meta_classifier=lr)

sclf2 =StackingClassifier(classifiers=[clf1,clf2,clf4], 
                          use_probas=True,
                          meta_classifier=lr)

sclf3 = StackingClassifier(classifiers=[clf1,clf3, clf4], 
                          use_probas=True,
                          meta_classifier=lr)

sclf4 = StackingClassifier(classifiers=[clf2,clf3, clf4], 
                          use_probas=True,
                          meta_classifier=lr)




#AdaBoost Classifier with Decision Tree
classifier = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=1),
    n_estimators=200)

#RandomForestClassifier
rf_model=RandomForestClassifier(n_estimators=80,random_state=2)


# build the lightgbm model
params = {'learning_rate':0.001}
LGBMclf = lgb.LGBMClassifier(learning_rate=0.001,random_state=10)

boosting_models={'Gradient Boosting Classifier':gb_model,
                 'XGBoost':xgb_model1,
                 'AdaBoost_DT':classifier,
                 'LightGBM':LGBMclf,
                 'CatBoost':catboost_model
                 }

stacking_models={'StackingClassifier(KNN,RF,GNB)':sclf1,
                 'StackingClassifier(KNN,RF,LR)':sclf2,
                 'StackingClassifier(KNN,GNB,LR)':sclf3,
                 'StackingClassifier(RF,GNB,LR)':sclf4
                 }

bagging_models={'Random Forest':rf_model}

voting_classifiers={'Voting Classifier(DT,KNN,LR)':eclf1,
                    'Voting Classifier(DT,KNN,GNB)':eclf2,
                    'Voting Classifier(DT,LR,GNB)':eclf3,
                    'Voting Classifier(KNN,LR,GNB)':eclf4
                    }

lr_model={'Logistic Regression':lr_model}

#Creating a dictionary of all the models
models={'Logistic Regression':lr_model,
        'Random Forest':rf_model,
        'Voting Classifier(Soft Voting)':eclf1,
        'StackingClassifier(KNN,RF,GNB)':sclf1,
        'XGBoost':xgb_model1,
        'AdaBoost_DT':classifier,'LightGBM':LGBMclf,
        }

all_models=dict(boosting_models)
all_models.update(stacking_models)
all_models.update(bagging_models)
all_models.update(voting_classifiers)
all_models.update(lr_model)

# from sklearn.preprocessing import RobustScaler
# robustscaler=RobustScaler()
# X=sampled_df.drop('Default_Status',axis=1)
# col_names=X.columns
# # X=robustscaler.fit_transform(X)
# y=sampled_df['Default_Status']
# train_X, test_X, train_y, test_y = train_test_split(X,y, random_state=20)

"""Without One Hot Encoding"""

X=sampled_df.drop('Default_Status',axis=1)
col_names=X.columns
y=sampled_df['Default_Status']
train_X, test_X, train_y, test_y = train_test_split(X,y, random_state=20)
rf_model.fit(train_X,train_y)
pred=rf_model.predict(test_X)
score=rf_model.score(test_X,test_y)
print(recall_score(test_y, pred))
print(precision_score(test_y, pred))
print(f1_score(test_y, pred))

my_colors = 'rgbkymc'  #red, green, blue, black, etc.
TOP10RF=pd.Series(rf_model.feature_importances_, index=col_names).nlargest(10)
print(TOP10RF.index)
(pd.Series(rf_model.feature_importances_, index=col_names)
   .nlargest(12)
   .plot(kind='bar',color=list('rgbkym')) )

top_12_rf=list(TOP10RF.index)

"""TRYING OHE"""

# sampled_df['GENDER']=sampled_df['GENDER'].replace({1:'Male',2:'Female'})
# sampled_df['MARITAL_STATUS']=sampled_df['MARITAL_STATUS'].replace({1:'Married',2:'Single',3:'Others'})
# sampled_df['EDUCATION']=sampled_df['EDUCATION'].replace({1:'Graduate',2:'University', 3:'High school', 4:'Others'})

# #We are one hot encoding these features because otherwise the model may assume that they have some linear relation with the target
# ohe_features=['EDUCATION','GENDER','MARITAL_STATUS']

# #Saving these dummies in a dataframe
# dummies=pd.get_dummies(sampled_df[ohe_features])

# sampled_df=pd.concat([sampled_df,dummies],axis=1)


# sampled_df.drop(ohe_features,axis=1,inplace=True)

lr=LogisticRegression(random_state=2,max_iter=400)
lr.fit(train_X,train_y)
pred=lr.predict(test_X)
score=lr.score(test_X,test_y)

print(recall_score(test_y, pred))
print(precision_score(test_y, pred))
print(f1_score(test_y, pred))

my_colors = 'rgbkymc'  #red, green, blue, black, etc.
TOP10LR=pd.Series(lr.coef_[0], index=col_names).nlargest(12)
(pd.Series(lr.coef_[0], index=col_names)
   .nlargest(12)
   .plot(kind='bar',color=list('rgbkym')) )

top_12_lr=list(TOP10LR.index)

top_12_lr

#Naive Bayes is clf3
clf3.fit(train_X,train_y)
pred=clf3.predict(test_X)
score=clf3.score(test_X,test_y)

from sklearn.inspection import permutation_importance

imps = permutation_importance(clf3, test_X, test_y)
importances = imps.importances_mean
std = imps.importances_std
indices = np.argsort(importances)[::-1]

# Print the feature ranking
rank=list()
name=list()
importance=list()
print("Feature ranking:")
for f in range(test_X.shape[1]):
    print("%d. %s (%f)" % (f + 1, col_names[indices[f]], importances[indices[f]]))
    rank.append(f + 1)
    name.append(col_names[indices[f]])
    importance.append(importances[indices[f]])

feat_imp=pd.DataFrame({'Rank':rank,'Name':name,'Importance':importance})

feat_imp.drop('Rank',axis=1,inplace=True)

plt.figure(figsize=(15, 15))
# plt.title("Feature importances")
feat_imp.plot.bar(x='Name', y='Importance', rot=90)

#Feature set

idealfeatures=['PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']
top_12_rf
top_12_lr
woe_features
rf_lr_combo=list(set(top_12_lr+top_12_rf))
rf_lr_common=list(set(top_12_rf).intersection(set(top_12_lr)))
allfeatures=list(col_names)


idealfeatures2=['PAY_1','PAY_2','PAY_6']
idealfeatures3=['PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','LIMIT_BAL', 'EDUCATION','GENDER','MARITAL_STATUS','PAY_AMT1',
       'PAY_AMT2','PAY_AMT4', 'PAY_AMT5','PAY_AMT3']
       
idealfeatures4=['PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','EDUCATION_Graduate', 'EDUCATION_High school', 'EDUCATION_Others',
       'EDUCATION_University', 'GENDER_Female', 'GENDER_Male',
       'MARITAL_STATUS_Married', 'MARITAL_STATUS_Others',
       'MARITAL_STATUS_Single','LIMIT_BAL',
       'PAY_AMT2','PAY_AMT4']
idealfeatures5=['PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6',	'LIMIT_BAL', 'EDUCATION_Graduate', 'EDUCATION_High school', 'EDUCATION_Others',
       'EDUCATION_University', 'GENDER_Female', 'GENDER_Male',
       'MARITAL_STATUS_Married', 'MARITAL_STATUS_Others',
       'MARITAL_STATUS_Single','PAY_AMT1',
       'PAY_AMT2']


idealfeatures6=['PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','LIMIT_BAL', 'GENDER_Female', 'GENDER_Male',
       'MARITAL_STATUS_Married', 'MARITAL_STATUS_Others',
       'MARITAL_STATUS_Single','PAY_AMT1',
       'PAY_AMT2','PAY_AMT4', 'PAY_AMT5','PAY_AMT3','PAY_AMT6','BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']


feature7=['AGE' ,'PAY_1', 'PAY_2', 'BILL_AMT1','PAY_AMT1','PAY_AMT2','Closeness_1','Closeness_2','Closeness_4','Closeness_5']

feature_set=['PAY_1', 'PAY_2','PAY_AMT1','PAY_AMT2','PAY_AMT3',
             'AGE', 'Closeness_1','Closeness_2','Closeness_3', 'Closeness_4','LIMIT_BAL']
allfeatures

rf_lr_common

from sklearn.utils.testing import ignore_warnings
from sklearn.exceptions import ConvergenceWarning
import sys
import warnings
import os
from sklearn.preprocessing import RobustScaler

#Evaluating the features on different ensemble models
def model_eval(dataframe,feature_set,models,scaling=False,ohe=False):
  accuracy=list()
  recall=list()
  precision=list()
  f1=list()

  # Defining a result table as a DataFrame roc,auc
  result_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])
  X=dataframe[feature_set]
  


  if ohe==True:
    
    X['GENDER']=X['GENDER'].replace({1:'Male',2:'Female'})
    X['MARITAL_STATUS']=X['MARITAL_STATUS'].replace({1:'Married',2:'Single',3:'Others'})
    X['EDUCATION']=X['EDUCATION'].replace({1:'Graduate',2:'University', 3:'High school', 4:'Others'})
    ohe_features=['EDUCATION','GENDER','MARITAL_STATUS']

    #Saving these dummies in a dataframe
    dummies=pd.get_dummies(X[ohe_features])

    X=pd.concat([X,dummies],axis=1)


    X.drop(ohe_features,axis=1,inplace=True)

    



  if scaling==True and ohe==False:
    robustscaler=RobustScaler()
    X=robustscaler.fit_transform(X)
  y=dataframe['Default_Status']
  train_X, test_X, train_y, test_y = train_test_split(X,y, random_state=20)
  print("Features Used Were:", feature_set)
  print("\n")

  all_cat_features=['MARITAL_STATUS','GENDER','EDUCATION']

  
  for name,model in models.items():

    # cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
   
    # cv_acc = cross_val_score(model,X,y,cv=cv,scoring="accuracy") # accuracy as scoring
    # cv_acc =np.mean(cv_acc)
    
    # cv_precision = cross_val_score(model,X,y,cv=cv,scoring="precision") # precision as scoring
    # cv_precision =np.mean(cv_precision)

    # cv_recall = cross_val_score(model,X,y,cv=cv,scoring="recall") # recall as scoring
    # cv_recall =np.mean(cv_recall)

    # cv_f1 = cross_val_score(model,X,y,cv=cv,scoring="f1") # f1 as scoring
    # cv_f1 = np.mean(cv_f1) # Average f1 score after 5 splits

    # accuracy.append(cv_acc)
    # recall.append(cv_recall)
    # precision.append(cv_precision)
    # f1.append(cv_f1)

    

    cat_feature=[feature for feature in feature_set if feature in all_cat_features]
    
    if name =='CatBoost':
      model.fit(train_X,train_y,cat_features=cat_feature)
    elif name=='LightGBM':
      model.fit(train_X,train_y)
    else:
      warnings.simplefilter("ignore")
      os.environ["PYTHONWARNINGS"] = "ignore"
      model.fit(train_X,train_y)

    pred=model.predict(test_X)
    #print(pred)
    score=model.score(test_X,test_y)
    accuracy.append(score)
    recall.append(recall_score(test_y, pred))
    precision.append(precision_score(test_y, pred))
    f1.append(f1_score(test_y, pred))

    #ROC AUC TABLE
    yproba = model.predict_proba(test_X)[::,1]
    fpr, tpr, _ = roc_curve(test_y,  yproba)
    auc = roc_auc_score(test_y, yproba)
    
    result_table = result_table.append({'classifiers':name,
                                        'fpr':fpr, 
                                        'tpr':tpr, 
                                        'auc':auc}, ignore_index=True)
    
  
  #Metrics Table
  Metrics=pd.DataFrame({"Accuracy":accuracy,"Precision":precision,'Recall':recall,'f1_score':f1},index=models.keys())
  print(Metrics.T)
  
  # pd.set_option('display.max_columns', None)
  #display(Metrics.T *100)
  fig,ax=plt.subplots(figsize=(10,8))
  Metrics.T.plot(kind='barh',ax=ax,width=0.6)
  ax.grid();
  ax.legend(fontsize=15,loc='center left', bbox_to_anchor=(1, 0.5),
          ncol=1 if len(models.keys())<=5 else 2, fancybox=True, shadow=True);
  
  print("\n")
  # for metric in['Accuracy','Precision','Recall','f1_score']:
  #   Metrics.plot.bar(x='Classifier', y=metric, rot=90)
    
  

  # Set name of the classifiers as index labels
  result_table.set_index('classifiers', inplace=True)

  fig = plt.figure(figsize=(8,6))

  for i in result_table.index:
    plt.plot(result_table.loc[i]['fpr'], 
             result_table.loc[i]['tpr'], 
             label="{}, AUC={:.3f}".format(i, result_table.loc[i]['auc'])
             )
    
  plt.plot([0,1], [0,1], color='orange', linestyle='--')

  plt.xticks(np.arange(0.0, 1.1, step=0.1))
  plt.xlabel("False Positive Rate", fontsize=15)

  plt.yticks(np.arange(0.0, 1.1, step=0.1))
  plt.ylabel("True Positive Rate", fontsize=15)

  plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)
  plt.legend(fontsize=13,prop={'size':13}, loc='center left', bbox_to_anchor=(1, 0.5),
          ncol=1)

  plt.show()


  #   # print(name)
  #   # print("Accuracy:",score)
  #   # print('Recall: ',recall_score(test_y, pred))
  #   # print('Precision: ',precision_score(test_y, pred))
  #   # print('F1 Score: ',f1_score(test_y, pred))
  #   # print("\n")
  #   # accuracy.append(score)
  #   # recall.append(recall_score(test_y, pred))
  #   # precision.append(precision_score(test_y, pred))
  #   # f1.append(f1_score(test_y, pred))
    
    
  # # Metrics=pd.DataFrame({"Accuracy":accuracy,"Precision":precision,'Recall':recall,'f1_score':f1},index=models.keys())
  # # # pd.set_option('display.max_columns', None)
  # # display(Metrics.T)
  # # print("\n")

model_eval(sampled_df,top_12_lr,stacking_models)

#model_eval(sampled_df,top_12_rf,stacking_models,scaling=True)

model_eval(sampled_df,woe_features,stacking_models,scaling=False)

#model_eval(sampled_df,woe_features,stacking_models,scaling=True)

model_eval(sampled_df,top_12_rf,voting_classifiers,scaling=False)

model_eval(sampled_df,woe_features,voting_classifiers)

model_eval(sampled_df,feature7,voting_classifiers)



X=sampled_df[top_12_rf]
y=sampled_df['Default_Status']
train_X, test_X, train_y, test_y = train_test_split(X,y, random_state=20)

#Model
eclf2.fit(train_X,train_y)

eclf2.predict_proba(test_X)

eclf2.predict_proba(test_X)[1]

eclf2.predict_proba(test_X)[:,1][1]

test_y.iloc[4]

test_y

import plotly.graph_objects as go

def gauge_chart(random_instance,model,dataframe,feature_set):
  X=dataframe[feature_set]
  y=dataframe['Default_Status']
  train_X, test_X, train_y, test_y = train_test_split(X,y, random_state=20)
  test_y

#Model
  model.fit(train_X,train_y)
  value=model.predict_proba(test_X)[:,1][random_instance]
  value=value*100
  print(f'The Actual Output is {int(test_y.iloc[random_instance])}')
  fig = go.Figure(go.Indicator(
    mode = "gauge+number+delta",
    value = value,
    domain = {'x': [0, 1], 'y': [0, 1]},
    title = {'text': "Default Risk % ", 'font': {'size': 24}},
    delta = {'reference': 50, 'increasing': {'color': "RebeccaPurple"}},
    gauge = {
        'axis': {'range': [None, 100], 'tickwidth': 1, 'tickcolor': "darkblue"},
        'bar': {'color': "darkblue"},
        'bgcolor': "white",
        'borderwidth': 2,
        'bordercolor': "gray",
        'steps': [
            {'range': [0, 25], 'color': 'cyan'},
            {'range': [25, 40], 'color': 'royalblue'}],
        'threshold': {
            'line': {'color': "red", 'width': 4},
            'thickness': 0.75,
            'value': 40}}))

  fig.update_layout(paper_bgcolor = "lavender", font = {'color': "darkblue", 'family': "Arial"})

  fig.show()

gauge_chart(42,eclf2,sampled_df,top_12_rf)

import pickle

#Let us save the 4 best models 

top_12_rf=['PAY_1', 'PAY_2', 'BILL_AMT2','BILL_AMT3', 'PAY_AMT1','PAY_AMT2', 'AGE', 
              'Closeness_2','Closeness_3']


sclf1.fit(sampled_df[top_12_rf],sampled_df['Default_Status'])
# =============================================================================
# sclf2.fit(sampled_df[top_12_rf],sampled_df['Default_Status'])
# sclf3.fit(sampled_df[top_12_rf],sampled_df['Default_Status'])
# =============================================================================
sclf4.fit(sampled_df[top_12_rf],sampled_df['Default_Status'])


eclf1.fit(sampled_df[top_12_rf],sampled_df['Default_Status'])
eclf2.fit(sampled_df[top_12_rf],sampled_df['Default_Status'])
catboost_model.fit(sampled_df[top_12_rf],sampled_df['Default_Status'])
rf_model.fit(sampled_df[top_12_rf],sampled_df['Default_Status'])


import pickle
#Saving the four models to a disk

pickle.dump(sclf1,open("stacking1.pkl","wb"))
# =============================================================================
# pickle.dump(sclf2,open("stacking2.pkl","wb"))
# pickle.dump(sclf3,open("stacking3.pkl","wb"))
# =============================================================================
pickle.dump(sclf4,open("stacking4.pkl","wb"))
pickle.dump(catboost_model,open("catboost.pkl","wb"))

pickle.dump(eclf1,open("voting1.pkl","wb"))
pickle.dump(eclf2,open("voting2.pkl","wb"))

# =============================================================================
# pickle.dump(rf_model,open("rf.pkl","wb"))
# =============================================================================

#!pip freeze > requirements.txt

# =============================================================================
# print(top_12_rf)
# 
# print(woe_features)
# 
# print(top_12_lr)
# 
# print(rf_lr_combo)
# 
# print(rf_lr_common)
# 
# print(idealfeatures)
# 
# print(idealfeatures3)
# 
# 
# =============================================================================
